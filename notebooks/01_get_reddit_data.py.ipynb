{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   datetime import datetime, timedelta\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from hashlib import sha256\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import praw\n",
    "import pytz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = os.environ['HOME']\n",
    "KEYS = f'{HOME}/config/db.json'\n",
    "ENV = 'dev'\n",
    "NOW = datetime.utcnow().replace(tzinfo=pytz.utc)\n",
    "TODAY = NOW.date()\n",
    "\n",
    "DATA_DIR = '../data' \n",
    "SQL_DIR = '../sql'\n",
    "OUT_DIR = '../output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = # read local keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Version 7.2.0 of praw is outdated. Version 7.4.0 was released Friday July 30, 2021.\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id=config['reddit']['client-id'],\n",
    "                     client_secret=config['reddit']['client-secret'],\n",
    "                     user_agent=config['reddit']['user-agent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blogpost Friday!\n",
      "Moronic Monday!\n",
      "Alternative for Cisco ACI\n",
      "Best pass through crimper?\n",
      "ASN Usage for Independent Sites\n",
      "OSPF design for Branch Office / Datacentre connectivity\n",
      "Cisco L2VPN xconnect to Huawei L2VPN VSI\n",
      "Creating a multi simcard network that is mobile\n",
      "Automation Optimization - simultaneous netmiko ssh connections\n",
      "Fiber distribution panel\n"
     ]
    }
   ],
   "source": [
    "hot_networking_posts = reddit.subreddit('networking').hot(limit=10)\n",
    "\n",
    "for post in hot_networking_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddits():\n",
    "    data = pd.read_csv(DATA_DIR+'/subreddits.csv')\n",
    "    data = data['subreddits'].str.replace(r'r/', '')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         networking\n",
       "1           sysadmin\n",
       "2             devops\n",
       "3      cybersecurity\n",
       "4    MachineLearning\n",
       "5        programming\n",
       "6           hardware\n",
       "Name: subreddits, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits = get_subreddits()\n",
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_epoch_to_datetime(epoch_time):\n",
    "    return datetime.utcfromtimestamp(epoch_time).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_hash(*var_args):\n",
    "        args = [str(arg) for arg in var_args]\n",
    "        hashed = sha256(''.join(args).encode('utf-8')).hexdigest()\n",
    "        return hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _delete_fields(comment_obj):\n",
    "    fields_to_del = ['_reddit','_replies','awarders',\n",
    "                    'user_reports','treatment_tags',\n",
    "                    'all_awardings','author_flair_richtext',\n",
    "                    'gildings', 'mod_reports']\n",
    "    for field in fields_to_del:\n",
    "        if field in comment_obj.keys():\n",
    "            del comment_obj[field]\n",
    "    return comment_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stringify_fields(comment_instance, comment_obj):\n",
    "    comment_obj.update({'author' : str(comment_instance.author)})\n",
    "    # comment_obj.update({'_submission': str(comment_instance.submission.title)})\n",
    "    comment_obj.update({'subreddit': str(comment_instance.subreddit)})\n",
    "    return comment_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_comment(comment_obj):\n",
    "    comment_obj['sha_id'] = _make_hash([\n",
    "            comment_obj['id'],\n",
    "            comment_obj['author'],\n",
    "            comment_obj['body'], \n",
    "            comment_obj['parent_id'],\n",
    "            comment_obj['subreddit'],\n",
    "            comment_obj['subreddit_id']])\n",
    "    comment_obj['created_at'] = _convert_epoch_to_datetime(comment_obj['created_utc'])\n",
    "    return comment_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit comment model: https://praw.readthedocs.io/en/latest/code_overview/models/comment.html\n",
    "def get_subreddit_comments(subreddit_name): \n",
    "    comments = []\n",
    "    print(f'\\n\\n--\\nObtaining data for r/{subreddit_name}...')\n",
    "    subreddit = reddit.subreddit(subreddit_name) # subreddit api\n",
    "    for comment_instance in subreddit.comments(limit=10):\n",
    "        comment_obj = vars(comment_instance)\n",
    "        submission_title = comment_instance.submission.title\n",
    "        print('submission_title = ', submission_title)\n",
    "        # stringify fields\n",
    "        comment_obj = _stringify_fields(comment_instance, comment_obj)\n",
    "        # delete fields\n",
    "        comment_obj = _delete_fields(comment_obj)\n",
    "        # update object with sha_id and created_at\n",
    "        comment_obj = _update_comment(comment_obj)\n",
    "        comments.append(comment_obj)\n",
    "    print(' - No. of comments: ', len(comments))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_max_col_length(comments_df):\n",
    "    '''\n",
    "    Get maximum length of column values\n",
    "    '''\n",
    "    comment_vals_as_str = comments_df.values.astype(str)\n",
    "    max_col_lengths = dict(zip(comments_df, measurer(comment_vals_as_str).max(axis=0)))\n",
    "\n",
    "    print('\\nmax column lengths:')\n",
    "    print(\"\\n\".join(\"{!r}: {!r}\".format(k, v) for k, v in max_col_lengths.items()) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, outpath):\n",
    "    print(f'\\nSaving data to {outpath}...')\n",
    "    \n",
    "    with open(outpath, 'w') as outfile:\n",
    "        for entry in data:\n",
    "            json.dump(entry, outfile)\n",
    "            outfile.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurer = np.vectorize(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/networking...\n",
      "submission_title =  Alternative for Cisco ACI\n",
      "submission_title =  Best pass through crimper?\n",
      "submission_title =  Alternative for Cisco ACI\n",
      "submission_title =  Alternative for Cisco ACI\n",
      "submission_title =  Best pass through crimper?\n",
      "submission_title =  Alternative for Cisco ACI\n",
      "submission_title =  ASN Usage for Independent Sites\n",
      "submission_title =  Creating a multi simcard network that is mobile\n",
      "submission_title =  Best pass through crimper?\n",
      "submission_title =  Creating a multi simcard network that is mobile\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/networking: 4.1614s\n",
      " - Time taken to write data for r/networking: 0.0000s\n",
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/sysadmin...\n",
      "submission_title =  Babysitting users. New responsibility for a Sysadmin.\n",
      "submission_title =  Windows internals 7th edition part 2 is out\n",
      "submission_title =  What operating systems do you all use outside of work?\n",
      "submission_title =  What operating systems do you all use outside of work?\n",
      "submission_title =  What operating systems do you all use outside of work?\n",
      "submission_title =  Babysitting users. New responsibility for a Sysadmin.\n",
      "submission_title =  Babysitting users. New responsibility for a Sysadmin.\n",
      "submission_title =  Prevent local admin from resetting users password in active directory\n",
      "submission_title =  Need opinon: SSD Raid-10 vs NVME RAID-1?\n",
      "submission_title =  365 Business Starter vs iCloud Custom domains?\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/sysadmin: 7.8787s\n",
      " - Time taken to write data for r/sysadmin: 0.0000s\n",
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/devops...\n",
      "submission_title =  Which databases do you hate the least at scale?\n",
      "submission_title =  Which databases do you hate the least at scale?\n",
      "submission_title =  Which databases do you hate the least at scale?\n",
      "submission_title =  Which databases do you hate the least at scale?\n",
      "submission_title =  Unit tests for hardened images\n",
      "submission_title =  Lambda's HTTP requests connection time out\n",
      "submission_title =  Lambda's HTTP requests connection time out\n",
      "submission_title =  Which databases do you hate the least at scale?\n",
      "submission_title =  Unit tests for hardened images\n",
      "submission_title =  Unit tests for hardened images\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/devops: 3.5336s\n",
      " - Time taken to write data for r/devops: 0.0000s\n",
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/cybersecurity...\n",
      "submission_title =  Bachelor's in cybersecurity, security+ certified and 1.5 years working in a helpdesk role. Any tips and advice for a successful interview process?\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Remember Do Not Trust Anyone\n",
      "submission_title =  Lets avoid the CEH & EC-Council\n",
      "submission_title =  Remember Do Not Trust Anyone\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/cybersecurity: 5.1751s\n",
      " - Time taken to write data for r/cybersecurity: 0.0000s\n",
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/MachineLearning...\n",
      "submission_title =  Illustrating Machine Learning with An Exercise for Business Students\n",
      "submission_title =  Deep\n",
      "submission_title =  [D] Cogram — Intuitive coding with plain language for machine learning\n",
      "submission_title =  LIdar and Camera fusion\n",
      "submission_title =  [D] Cogram — Intuitive coding with plain language for machine learning\n",
      "submission_title =  [D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.\n",
      "submission_title =  [D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.\n",
      "submission_title =  [D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.\n",
      "submission_title =  [D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.\n",
      "submission_title =  [D] How OpenAI Sold its Soul for $1 Billion: The company behind GPT-3 and Codex isn’t as open as it claims.\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/MachineLearning: 4.4359s\n",
      " - Time taken to write data for r/MachineLearning: 0.0000s\n",
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/programming...\n",
      "submission_title =  Hiring Developers: How to avoid the best\n",
      "submission_title =  Scriptimate: an open source tool to create SVG animations in a scripting way\n",
      "submission_title =  Hiring Developers: How to avoid the best\n",
      "submission_title =  Hiring Developers: How to avoid the best\n",
      "submission_title =  UUINext: a modern .NET library to generate database friendly or name based UUID/GUID based on the latest RFC draft [crosspost from r/csharp]\n",
      "submission_title =  Hiring Developers: How to avoid the best\n",
      "submission_title =  3 Variants of Python Dictionaries That Make Your Coding Easier\n",
      "submission_title =  Hiring Developers: How to avoid the best\n",
      "submission_title =  Scriptimate: an open source tool to create SVG animations in a scripting way\n",
      "submission_title =  Hiring Developers: How to avoid the best\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/programming: 16.5341s\n",
      " - Time taken to write data for r/programming: 0.0000s\n",
      "\n",
      "\n",
      "--\n",
      "Obtaining data for r/hardware...\n",
      "submission_title =  ELI5: Why does it seem like Cinebench is now the standard benchmark for CPUs?\n",
      "submission_title =  ELI5: Why does it seem like Cinebench is now the standard benchmark for CPUs?\n",
      "submission_title =  ELI5: Why does it seem like Cinebench is now the standard benchmark for CPUs?\n",
      "submission_title =  ELI5: Why does it seem like Cinebench is now the standard benchmark for CPUs?\n",
      "submission_title =  [HUB] Are GPU Prices Getting Worse, Again!? August 2021 Update\n",
      "submission_title =  [HUB] Are GPU Prices Getting Worse, Again!? August 2021 Update\n",
      "submission_title =  [HUB] Are GPU Prices Getting Worse, Again!? August 2021 Update\n",
      "submission_title =  ELI5: Why does it seem like Cinebench is now the standard benchmark for CPUs?\n",
      "submission_title =  What are the possible Hardware Interfaces that a modern type-C hub can use?\n",
      "submission_title =  ELI5: Why does it seem like Cinebench is now the standard benchmark for CPUs?\n",
      " - No. of comments:  10\n",
      " - Time taken to get data from r/hardware: 11.3420s\n",
      " - Time taken to write data for r/hardware: 0.0000s\n",
      "\n",
      "\n",
      "Total run time: 54.0916s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for idx, subreddit in enumerate(subreddits):\n",
    "    sub_start = time.time()\n",
    "    subreddit_comments = get_subreddit_comments(subreddit)\n",
    "    sub_end = time.time()\n",
    "    print(f' - Time taken to get data from r/{subreddit}: {(sub_end-sub_start):.4f}s')\n",
    "    \n",
    "    write_start = time.time()\n",
    "    save_data(subreddit_comments, f'{OUT_DIR}/{subreddit}.jsonl')\n",
    "    write_end = time.time()\n",
    "    print(f' - Time taken to write data for r/{subreddit}: {(write_end-write_start):.4f}s')\n",
    "    \n",
    "    comments_df = pd.json_normalize(subreddit_comments)\n",
    "end = time.time()\n",
    "\n",
    "# print maximum length of column values\n",
    "print(f'\\n\\nTotal run time: {(end-start):0.4f}s')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('3.8.0': pyenv)",
   "language": "python",
   "name": "python38064bit380pyenveef7d5a615c845119a299c3dc621f66c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
